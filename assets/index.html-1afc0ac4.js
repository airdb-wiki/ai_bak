import{_ as r,M as i,p as o,q as h,R as e,t as n,N as a,a1 as s}from"./framework-5866ffd3.js";const c={},l=s('<h1 id="fine-tuning" tabindex="-1"><a class="header-anchor" href="#fine-tuning" aria-hidden="true">#</a> Fine Tuning</h1><h2 id="什么是-fine-tuning" tabindex="-1"><a class="header-anchor" href="#什么是-fine-tuning" aria-hidden="true">#</a> 什么是 fine-tuning？</h2><p>Fine-tuning（微调）是一种迁移学习方法，它可以加快并优化模型的学习效率。迁移学习考虑到大部分数据或任务都存在相关性，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型[1]。在实践中，由于数据集不够大，很少有人从头开始训练网络。常见的做法是使用预训练的网络（例如在ImageNet上训练的分类1000类的网络）来重新fine-tuning（也叫微调），或者当做特征提取器[3]。</p><p>当训练数据集较小（小于参数数量）时，直接训练CNN可能会导致过拟合，从而影响模型的泛化能力。因此，在实践中更经常的做法是通过对我们拥有的较小数据集进行训练（即反向传播），对现有网络进行微调。这些网络通常是在像ImageNet这样的大型数据集上进行训练的[2]。通过使用预训练的网络并进行微调，可以借用已经学到的知识提高新模型的性能。</p><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference" aria-hidden="true">#</a> Reference</h2>',5),d={href:"https://www.zhihu.com/question/24458804",target:"_blank",rel:"noopener noreferrer"},u={href:"https://zhuanlan.zhihu.com/p/35890660",target:"_blank",rel:"noopener noreferrer"},f={href:"https://blog.csdn.net/weixin_42137700/article/details/82107208",target:"_blank",rel:"noopener noreferrer"};function _(p,g){const t=i("ExternalLinkIcon");return o(),h("div",null,[l,e("p",null,[n('[1] "要了解什么是Fine-tuning（微调），就要先提到迁移学习概念。 迁移学习 考虑到大部分数据或任务都是存在相关性的，所以通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不 ..." URL: '),e("a",d,[n("https://www.zhihu.com/question/24458804"),a(t)])]),e("p",null,[n('[2] "在小数据集（小于参数数量）上训练CNN会极大地影响CNN泛化的能力，通常会导致过度拟合。. 因此，在实践中更经常的是，通过对我们拥有的较小数据集进行训练（即反向传播），对现有网络进行微调，这些网络是在像ImageNet这样的大型数据集上进行训练的，以 ..." URL: '),e("a",u,[n("https://zhuanlan.zhihu.com/p/35890660"),a(t)])]),e("p",null,[n('[3] "什么是fine-tuning？ 在实践中，由于数据集不够大，很少有人从头开始训练网络。常见的做法是使用预训练的网络（例如在ImageNet上训练的分类1000类的网络）来重新fine-tuning（也叫微调），或者当做特征提取器。 以下是常见的两类迁移学习场景：1 卷积网络当做特征提取器。" URL: '),e("a",f,[n("https://blog.csdn.net/weixin_42137700/article/details/82107208"),a(t)])])])}const N=r(c,[["render",_],["__file","index.html.vue"]]);export{N as default};
